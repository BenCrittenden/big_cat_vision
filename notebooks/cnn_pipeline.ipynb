{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fd6514-083e-4b03-b24d-165ae677ee33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e69f8d7-72c1-4106-9354-d0ebe278ad1f",
   "metadata": {},
   "source": [
    "# load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69490a2e-80c8-4dd9-af82-ee0628c11bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the tranforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.Resize(64),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "valid_test_transform = transforms.Compose([\n",
    "    #transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06003862-d37e-4440-83ab-ef0c39ff8bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data from the specified dirs\n",
    "\n",
    "train_data = datasets.ImageFolder(\"../data/train\", transform=train_transform)\n",
    "valid_data = datasets.ImageFolder(\"../data/valid\", transform=valid_test_transform)\n",
    "test_data = datasets.ImageFolder(\"../data/test\", transform=valid_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756445a9-ca89-4bf5-87ec-a3d7aaa166df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify some parameters\n",
    "# num_workers = 0\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e451a14-17fe-4df6-bd28-89c541abc0aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=batch_size,\n",
    "    # num_workers=num_workers,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    train_data, \n",
    "    batch_size=5, #because there's only 5 images in the validation dir\n",
    "    # num_workers=num_workers\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, \n",
    "    batch_size=5, #because there's only 5 images in the test dir\n",
    "    # num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82460956-c1e4-4f3e-8bc5-dbe87da192ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classes = utils.listdir_ignore_hidden(\"../data/train\")\n",
    "classes = sorted([c for c in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29f4a3-1294-497e-aead-a8cde7afb829",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.show_batch(train_loader, batch_size, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010fb887-915f-4f66-9b8b-1cdc7147108b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5acd8b5-478b-44bd-a2b4-ed7a1bff2075",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate dims of first fc layer\n",
    "\n",
    "image_height_width = images[0].shape[-1]\n",
    "n_pool_layers = 3\n",
    "pool_size = 2\n",
    "n_filters_final_layer = 64\n",
    "\n",
    "fc_layer_units = (\n",
    "    n_filters_final_layer \n",
    "    * (image_height_width / (pool_size ** n_pool_layers))\n",
    "    * (image_height_width / (pool_size ** n_pool_layers))\n",
    ")\n",
    "\n",
    "print(fc_layer_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96af8fd4-b9fb-4c0e-8168-47b194acbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "(image_height_width / (pool_size ** n_pool_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c513b-b529-4795-a16a-41bb836bf519",
   "metadata": {},
   "source": [
    "# Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346f412-732a-4fa1-b6d0-1b031f5ce58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## TODO: Define the NN architecture\n",
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=16,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=16,\n",
    "            out_channels=32,\n",
    "            kernel_size=5,\n",
    "            padding=2,\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32,\n",
    "            out_channels=64,\n",
    "            kernel_size=7,\n",
    "            padding=3,\n",
    "        )\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 28 * 28, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "        x = self.maxpool(F.relu(self.conv3(x)))\n",
    "        \n",
    "        x = x.view(-1, 64 * 28 * 28)\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = cnn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab595cf-4e53-4d84-b90d-3eae869dd189",
   "metadata": {},
   "source": [
    "# Build the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b1a99-571f-49a1-95f6-6df8f9da7728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 10\n",
    "steps = 0\n",
    "\n",
    "train_losses, valid_losses, accuracies = [], [], []\n",
    "for e in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        log_ps = model.forward(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    " \n",
    "    else:\n",
    "        valid_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in valid_loader:\n",
    "                log_ps = model.forward(images)\n",
    "                valid_loss += criterion(log_ps, labels)\n",
    "                \n",
    "                ps = torch.exp(log_ps)\n",
    "                top_p, top_class = ps.topk(1, dim=1)\n",
    "                equals = top_class == labels.view(*top_class.shape)\n",
    "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "                \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        valid_losses.append(valid_loss/len(valid_loader))\n",
    "\n",
    "        print(\n",
    "            \"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "            \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "            \"Validation Loss: {:.3f}.. \".format(valid_losses[-1]),\n",
    "            \"Validation Accuracy: {:.3f}\".format(accuracy/len(valid_loader))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cb32c-dea8-40b2-ae02-bdf4e36eaca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_np = np.array(images)\n",
    "img = (images_np[4] / 2) + 0.5  # unnormalize\n",
    "plt.imshow(np.transpose(img, (1, 2, 0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c9178f-714e-4bbf-b80f-d122e5ac1898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
